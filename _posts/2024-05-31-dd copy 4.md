---
title: Word Embedding
description: One-hot encoding, Word Embedding(CBOW & Skip-gram), GloVe
#author: RogersHun
date: 2024-06-25 11:33:00 +0800
categories: [AI, NLP]
tags: [typography] #밑에 태그
pin: true
math: true
mermaid: true 
---

> ### Intro

2013년, Google 사에서 다음과 같은 연구가 발표되었다.

[Efficient Estimation of Word Representations in Vector Space, Mikolov et al., 2013](https://www.cambridge.org/core/journals/apsipa-transactions-on-signal-and-information-processing/article/evaluating-word-embedding-models-methods-and-experimental-results/EDF43F837150B94E71DBB36B28B85E79)

본 논문의 Word2Vec model은 CBOW와 Skip-gram Arichitecture를 통해 효율적으로 단어의 벡터 표현을 학습하는 방법을 제시하였다. 이는 NLP(자연어처리) 분야에 큰 영향을 미쳤고, 현재까지도 많은 연구 및 산업에 사용되는 BERT, GPT 등 Transformer model의 근간이 되었다.

해당 글에서는 Word Embedding 이전에 사용되었던 **One-hot encoding**과, **Word Embedding(with CBOW & Skip-gram)**, Word2Vec과 가장 많이 쓰이던 단어 임베딩 기술인 **GloVe**에 대해 다뤄보겠다.



> ### One-Hot Encoding
한줄 요약 : 각 단어를 고유한 벡터로 표현하는 방법으로, 생성되는 벡터에서 하나의 요소만 1이고 나머지는 0으로 표현되는 벡터를 생성하는 과정

NLP, 이미지처리, 신호처리 등의 분야에서 사용될 수 있으며 NLP에서는 주로 텍스트 데이터를 수치 데이터로 변환할 때 사용된다. Word Embedding 이전, 구현이 간단하고 명확하여 가장 많이 쓰였었다. 
   
벡터를 생성하는 방식은 다음과 같다.   
예를 들어, ['Ronaldo','Messi','Son','Ohtani'] 라는 단어집이 있다. 각 단어의 위치 (인덱스 즉, 순서를 부여하여 할당함 ex.Ronaldo -> 0, Messi -> 1) 에 해당하는 부분만 1, 나머지는 0으로 채워 벡터를 생성한다.   
```
'Ronaldo' = [1,0,0,0]
'Messi' = [0,1,0,0]    
'Son' = [0,0,1,0]    
'Ohtani' = [0,0,0,1]   
```
단어 집합의 크기가 4 이므로, 각 단어는 4차원 벡터로 표현된다. 이렇게 생성된 벡터는 텍스트 분류, 생성, 변역, 임베딩 레이어의 입력값으로 사용된다.   
   
간단하고 사용도 편하지만, 딥러닝 모델등에 넣기에는 데이터의 크기가 발목을 잡게 된다.    
단어집의 크기가 지금은 4개지만 <u> 크기가 커질수록 벡터의 크기도 커지는</u>데, <u>커질수록 0의 값은 많아지지만 1의 값은 단 하나</u>이다.   
즉, **<u>희소한 벡터를 생성</u>** 한다라고 말할 수 있다.    
또한, 단어 간의 유사성을 반영하지 못하는데, 예를들어, 'Football', 'Soccer'는 의미적으로 유사하지만 One-hot encoding에서는 유사한 것을 표현하지 못하고 아예 다른 벡터로써 표현된다.    이러한 문제점들을 개선한 것이 **Word Embedding** 방식이다.

> **cf.** 텍스트 데이터를 수치 데이터로 바꾸는 이유?   
> 인간만 이해할 수 있는 자연어(인간 언어)를 컴퓨터가 이해할 수 있는 수치로 바꾸는 과정이 필요하기 때문.
{: .prompt-tip }
   
   
### Word Embedding
Upload Waiting